{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maDaEM8Y4Xao"
      },
      "source": [
        "# YOLOv8-Nano Detection Model Training\n",
        "## Cattle Breed Recognition System - Stage 1: Animal Detection\n",
        "\n",
        "This notebook trains a YOLOv8-Nano model for detecting cattle and buffaloes in images.\n",
        "\n",
        "**Model Specifications:**\n",
        "- Model: YOLOv8-Nano (smallest, fastest)\n",
        "- Input Size: 416x416\n",
        "- Output: Bounding box (x, y, w, h) for animal detection\n",
        "- Final Size: ~5 MB (after INT8 quantization)\n",
        "- Inference: ~20ms on mobile\n",
        "\n",
        "**Author:** SIH 2025 Team\n",
        "**Problem Statement:** SIH25004 - Image-based Breed Recognition for Cattle and Buffaloes of India"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIX: PyTorch 2.6+ compatibility (RUN THIS FIRST!)\n",
        "import torch\n",
        "import functools\n",
        "\n",
        "_original_torch_load = torch.load\n",
        "\n",
        "@functools.wraps(_original_torch_load)\n",
        "def _patched_load(f, map_location=None, pickle_module=None, *, weights_only=None, mmap=None, **kwargs):\n",
        "    return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module,\n",
        "                                 weights_only=False, mmap=mmap, **kwargs)\n",
        "\n",
        "torch.load = _patched_load\n",
        "print(\"‚úÖ PyTorch patch applied! Now run the rest of the notebook.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm9Qyv5e5Pki",
        "outputId": "0220ec4b-bddb-4b56-fd36-b5a01a14ecaa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PyTorch patch applied! Now run the rest of the notebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Install YOLOv8\n",
        "!pip install ultralytics==8.0.196 -q\n",
        "!pip install roboflow -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSNfBBKI6ED8",
        "outputId": "eafbd6b4-d154-4990-c6be-6a7eab377a81"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m631.1/631.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m94.0/94.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m137.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for data storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "BASE_PATH = '/content/drive/MyDrive/cattle_breed_recognition'\n",
        "...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asSASMFc6q5f",
        "outputId": "0d7a44c3-ef7e-48eb-c4a1-1a8058df7d21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load ZIP files from Google Drive backup\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import random\n",
        "\n",
        "backup_folder = '/content/drive/MyDrive/cattle_breed_recognition/dataset_backup'\n",
        "print(\"üìÇ Loading ZIP files from Google Drive backup...\")\n",
        "\n",
        "if os.path.exists(backup_folder):\n",
        "    for f in os.listdir(backup_folder):\n",
        "        if f.endswith('.zip'):\n",
        "            src = os.path.join(backup_folder, f)\n",
        "            dst = f'/content/{f}'\n",
        "            if not os.path.exists(dst):\n",
        "                size_mb = os.path.getsize(src) / (1024 * 1024)\n",
        "                print(f\"   Loading: {f} ({size_mb:.1f} MB)\")\n",
        "                shutil.copy2(src, dst)\n",
        "            else:\n",
        "                print(f\"   Already exists: {f}\")\n",
        "    print(\"‚úÖ ZIP files loaded!\")\n",
        "else:\n",
        "    print(\"‚ùå Backup folder not found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PSYFik77lB5",
        "outputId": "8b760e70-1165-46c9-850a-53179fc53ebc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading ZIP files from Google Drive backup...\n",
            "‚ùå Backup folder not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Google Drive contents\n",
        "import os\n",
        "\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "print(\"üìÅ Google Drive contents:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    for item in sorted(os.listdir(drive_path)):\n",
        "        item_path = os.path.join(drive_path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"   üìÇ {item}/\")\n",
        "        else:\n",
        "            size_mb = os.path.getsize(item_path) / (1024 * 1024)\n",
        "            print(f\"   üìÑ {item} ({size_mb:.1f} MB)\")\n",
        "else:\n",
        "    print(\"   (empty or not accessible)\")\n",
        "\n",
        "# Check if cattle_breed_recognition folder exists\n",
        "cattle_path = '/content/drive/MyDrive/cattle_breed_recognition'\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "if os.path.exists(cattle_path):\n",
        "    print(f\"‚úÖ cattle_breed_recognition folder exists\")\n",
        "    print(\"Contents:\")\n",
        "    for item in sorted(os.listdir(cattle_path)):\n",
        "        print(f\"   üìÇ {item}/\")\n",
        "else:\n",
        "    print(\"‚ùå cattle_breed_recognition folder not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmfSPsZ98B2K",
        "outputId": "c90c0557-5947-4adb-b75e-1c7ac1bb5eec"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Google Drive contents:\n",
            "============================================================\n",
            "   üìÇ Classroom/\n",
            "   üìÇ Colab Notebooks/\n",
            "   üìÑ SHIVAM JAISWAL.png (0.3 MB)\n",
            "   üìÑ Screenshot_20260105_210835.jpg (0.2 MB)\n",
            "   üìÇ cattle_breed_recognition/\n",
            "\n",
            "============================================================\n",
            "‚úÖ cattle_breed_recognition folder exists\n",
            "Contents:\n",
            "   üìÇ 01_yolov8_detection_training.ipynb/\n",
            "   üìÇ 02_efficientnet_classification_training.ipynb/\n",
            "   üìÇ 03_classification_training.ipynb/\n",
            "   üìÇ data/\n",
            "   üìÇ models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE TO GOOGLE DRIVE IMMEDIATELY\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create backup folder\n",
        "backup_folder = '/content/drive/MyDrive/cattle_breed_recognition/dataset_backup'\n",
        "os.makedirs(backup_folder, exist_ok=True)\n",
        "\n",
        "print(\"üíæ Saving ZIP files to Google Drive...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Find ZIP files in /content/\n",
        "zip_files = [f for f in os.listdir('/content/') if f.endswith('.zip')]\n",
        "\n",
        "if zip_files:\n",
        "    for zip_name in zip_files:\n",
        "        src = f'/content/{zip_name}'\n",
        "        dst = f'{backup_folder}/{zip_name}'\n",
        "        size_mb = os.path.getsize(src) / (1024 * 1024)\n",
        "        print(f\"   Saving: {zip_name} ({size_mb:.1f} MB)\")\n",
        "        shutil.copy2(src, dst)\n",
        "    print(\"=\" * 60)\n",
        "    print(\"‚úÖ All ZIP files saved to Google Drive!\")\n",
        "    print(f\"üìÇ Location: {backup_folder}\")\n",
        "else:\n",
        "    print(\"‚ùå No ZIP files found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m87l70bAgWz",
        "outputId": "fce29074-1606-44fa-ec0a-555239913958"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving ZIP files to Google Drive...\n",
            "============================================================\n",
            "   Saving: cow breed-3.zip (0.5 MB)\n",
            "   Saving: cow breed-2.zip (0.2 MB)\n",
            "   Saving: cow breed-1.zip (603.5 MB)\n",
            "============================================================\n",
            "‚úÖ All ZIP files saved to Google Drive!\n",
            "üìÇ Location: /content/drive/MyDrive/cattle_breed_recognition/dataset_backup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternative: Use unzip command for extraction\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "print(\"üì¶ Extracting ZIP files using unzip command...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create temp directory\n",
        "os.makedirs('/content/temp_images', exist_ok=True)\n",
        "\n",
        "zip_files = ['/content/cow breed-1.zip', '/content/cow breed-2.zip', '/content/cow breed-3.zip']\n",
        "\n",
        "for zip_path in zip_files:\n",
        "    if os.path.exists(zip_path):\n",
        "        size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "        print(f\"\\nExtracting: {os.path.basename(zip_path)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "        try:\n",
        "            # Use unzip command\n",
        "            result = subprocess.run(\n",
        "                ['unzip', '-o', '-q', zip_path, '-d', '/content/temp_images'],\n",
        "                capture_output=True,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(f\"   ‚úÖ Extracted successfully\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è Warning: {result.stderr[:100]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ùå Error: {e}\")\n",
        "\n",
        "# Count extracted images\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä Counting extracted images...\")\n",
        "\n",
        "import glob\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.JPEG', '*.PNG']\n",
        "all_images = []\n",
        "\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(glob.glob(f'/content/temp_images/**/{ext}', recursive=True))\n",
        "\n",
        "print(f\"‚úÖ Found {len(all_images)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckTxlU4RDCQg",
        "outputId": "b4d86ad5-54ad-4ba8-cb5b-d935296ed0ff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Extracting ZIP files using unzip command...\n",
            "============================================================\n",
            "\n",
            "Extracting: cow breed-1.zip (603.5 MB)\n",
            "   ‚ö†Ô∏è Warning: error [/content/cow breed-1.zip]:  missing 24117248 bytes in zipfile\n",
            "  (attempting to process anyway\n",
            "\n",
            "Extracting: cow breed-2.zip (0.2 MB)\n",
            "   ‚úÖ Extracted successfully\n",
            "\n",
            "Extracting: cow breed-3.zip (0.5 MB)\n",
            "   ‚úÖ Extracted successfully\n",
            "\n",
            "============================================================\n",
            "üìä Counting extracted images...\n",
            "‚úÖ Found 34 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "zip_path = '/content/cow breed-1.zip'\n",
        "if os.path.exists(zip_path):\n",
        "    size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "    print(f\"üì¶ cow breed-1.zip: {size_mb:.1f} MB\")\n",
        "\n",
        "    if size_mb > 620:\n",
        "        print(\"‚úÖ File size looks correct!\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è File may still be incomplete. Expected ~626 MB\")\n",
        "else:\n",
        "    print(\"‚ùå File not found\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O9KtLlldEeSs",
        "outputId": "3d8dc667-5dc0-4059-c8ca-5ac5334b76f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ cow breed-1.zip: 626.5 MB\n",
            "‚úÖ File size looks correct!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the complete ZIP file to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "backup_folder = '/content/drive/MyDrive/cattle_breed_recognition/dataset_backup'\n",
        "os.makedirs(backup_folder, exist_ok=True)\n",
        "\n",
        "print(\"üíæ Saving complete ZIP file to Google Drive...\")\n",
        "\n",
        "src = '/content/cow breed-1.zip'\n",
        "dst = f'{backup_folder}/cow breed-1.zip'\n",
        "size_mb = os.path.getsize(src) / (1024 * 1024)\n",
        "print(f\"   Saving: cow breed-1.zip ({size_mb:.1f} MB)\")\n",
        "shutil.copy2(src, dst)\n",
        "\n",
        "print(\"‚úÖ Saved to Google Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG23yqnaKWdP",
        "outputId": "d53c3c98-b79f-4a50-f75d-01daecceed30"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving complete ZIP file to Google Drive...\n",
            "   Saving: cow breed-1.zip (626.5 MB)\n",
            "‚úÖ Saved to Google Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all ZIP files and create dataset\n",
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "DATASET_PATH = '/content/detection_dataset'\n",
        "\n",
        "# Clean up previous extraction\n",
        "if os.path.exists('/content/temp_images'):\n",
        "    shutil.rmtree('/content/temp_images')\n",
        "os.makedirs('/content/temp_images', exist_ok=True)\n",
        "\n",
        "# Create dataset directories\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    os.makedirs(f'{DATASET_PATH}/{split}/images', exist_ok=True)\n",
        "    os.makedirs(f'{DATASET_PATH}/{split}/labels', exist_ok=True)\n",
        "\n",
        "print(\"üì¶ Extracting all ZIP files...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "zip_files = ['/content/cow breed-1.zip', '/content/cow breed-2.zip', '/content/cow breed-3.zip']\n",
        "\n",
        "for zip_path in zip_files:\n",
        "    if os.path.exists(zip_path):\n",
        "        size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "        print(f\"   Extracting: {os.path.basename(zip_path)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "        result = subprocess.run(\n",
        "            ['unzip', '-o', '-q', zip_path, '-d', '/content/temp_images'],\n",
        "            capture_output=True, text=True\n",
        "        )\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"      ‚úÖ Done\")\n",
        "        else:\n",
        "            print(f\"      ‚ö†Ô∏è {result.stderr[:50] if result.stderr else 'OK'}\")\n",
        "\n",
        "# Count images\n",
        "image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.JPEG', '*.PNG']\n",
        "all_images = []\n",
        "for ext in image_extensions:\n",
        "    all_images.extend(glob.glob(f'/content/temp_images/**/{ext}', recursive=True))\n",
        "\n",
        "print(f\"\\n‚úÖ Found {len(all_images)} images\")\n",
        "\n",
        "# Split and copy\n",
        "if len(all_images) > 0:\n",
        "    random.shuffle(all_images)\n",
        "    n = len(all_images)\n",
        "    train_images = all_images[:int(n*0.7)]\n",
        "    val_images = all_images[int(n*0.7):int(n*0.85)]\n",
        "    test_images = all_images[int(n*0.85):]\n",
        "\n",
        "    def copy_images(images, split):\n",
        "        print(f\"   Copying {len(images)} to {split}...\")\n",
        "        for i, img_path in enumerate(images):\n",
        "            ext = os.path.splitext(img_path)[1]\n",
        "            dest = f'{DATASET_PATH}/{split}/images/image_{i}{ext}'\n",
        "            shutil.copy(img_path, dest)\n",
        "\n",
        "            label_dest = f'{DATASET_PATH}/{split}/labels/image_{i}.txt'\n",
        "            with open(label_dest, 'w') as f:\n",
        "                f.write('0 0.5 0.5 0.8 0.8\\n')\n",
        "\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"      Progress: {i + 1}/{len(images)}\")\n",
        "\n",
        "    copy_images(train_images, 'train')\n",
        "    copy_images(val_images, 'valid')\n",
        "    copy_images(test_images, 'test')\n",
        "\n",
        "    shutil.rmtree('/content/temp_images', ignore_errors=True)\n",
        "\n",
        "    print(f\"\\n‚úÖ Dataset created:\")\n",
        "    print(f\"   üìÇ Train: {len(train_images)} images\")\n",
        "    print(f\"   üìÇ Valid: {len(val_images)} images\")\n",
        "    print(f\"   üìÇ Test: {len(test_images)} images\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGGFJs9IKcq2",
        "outputId": "69ce6fab-b988-42e5-aa13-4c612bea5ea6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Extracting all ZIP files...\n",
            "============================================================\n",
            "   Extracting: cow breed-1.zip (626.5 MB)\n",
            "      ‚úÖ Done\n",
            "   Extracting: cow breed-2.zip (0.2 MB)\n",
            "      ‚úÖ Done\n",
            "   Extracting: cow breed-3.zip (0.5 MB)\n",
            "      ‚úÖ Done\n",
            "\n",
            "‚úÖ Found 2201 images\n",
            "   Copying 1540 to train...\n",
            "      Progress: 500/1540\n",
            "      Progress: 1000/1540\n",
            "      Progress: 1500/1540\n",
            "   Copying 330 to valid...\n",
            "   Copying 331 to test...\n",
            "\n",
            "‚úÖ Dataset created:\n",
            "   üìÇ Train: 1540 images\n",
            "   üìÇ Valid: 330 images\n",
            "   üìÇ Test: 331 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE: PyTorch Fix + Create data.yaml + Train Model\n",
        "# Run this entire cell\n",
        "\n",
        "# Step 1: Apply PyTorch Fix\n",
        "import torch\n",
        "import functools\n",
        "\n",
        "_original_torch_load = torch.load\n",
        "\n",
        "@functools.wraps(_original_torch_load)\n",
        "def _patched_load(f, map_location=None, pickle_module=None, *, weights_only=None, mmap=None, **kwargs):\n",
        "    return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module,\n",
        "                                 weights_only=False, mmap=mmap, **kwargs)\n",
        "\n",
        "torch.load = _patched_load\n",
        "print(\"‚úÖ PyTorch patch applied!\")\n",
        "\n",
        "# Step 2: Create data.yaml\n",
        "import os\n",
        "\n",
        "yaml_content = \"\"\"names:\n",
        "- cattle\n",
        "nc: 1\n",
        "path: /content/detection_dataset\n",
        "test: test/images\n",
        "train: train/images\n",
        "val: valid/images\n",
        "\"\"\"\n",
        "\n",
        "yaml_path = '/content/detection_dataset/data.yaml'\n",
        "with open(yaml_path, 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "\n",
        "print(\"‚úÖ data.yaml created!\")\n",
        "\n",
        "# Step 3: Train model\n",
        "print(\"\\nüöÄ Starting YOLOv8-Nano Training...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"üìä Dataset: 1,540 train, 330 valid, 331 test\")\n",
        "print(f\"üñ•Ô∏è Device: CPU\")\n",
        "print(f\"üìê Image size: 320x320\")\n",
        "print(f\"üîÑ Epochs: 20\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "results = model.train(\n",
        "    data=yaml_path,\n",
        "    epochs=20,\n",
        "    imgsz=320,\n",
        "    batch=16,\n",
        "    name='cattle_detector',\n",
        "    project='cattle_breed_recognition',\n",
        "    device='cpu',\n",
        "    patience=5,\n",
        "    save=True,\n",
        "    plots=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "0l5xrnS-P4iz",
        "outputId": "ab5403e0-1cc8-4350-f5a8-70cbbf2e4462"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ PyTorch patch applied!\n",
            "‚úÖ data.yaml created!\n",
            "\n",
            "üöÄ Starting YOLOv8-Nano Training...\n",
            "============================================================\n",
            "üìä Dataset: 1,540 train, 330 valid, 331 test\n",
            "üñ•Ô∏è Device: CPU\n",
            "üìê Image size: 320x320\n",
            "üîÑ Epochs: 20\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.23M/6.23M [00:00<00:00, 67.4MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RecursionError",
          "evalue": "maximum recursion depth exceeded",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3733214188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yolov8n.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m results = model.train(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_ckpt_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mattempt_load_one_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0;34m\"\"\"Loads a single model weights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m     \u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_safe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# load ckpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mDEFAULT_CFG_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_args'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# combine model and default args, preferring model args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ema'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# FP32 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0;34m'ultralytics.yolo.v8'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ultralytics.models.yolo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                 'ultralytics.yolo.data': 'ultralytics.data'}):  # for legacy 8.0 Classify and Pose models\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m  \u001b[0;31m# load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# e.name is missing module name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3733214188.py\u001b[0m in \u001b[0;36m_patched_load\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_original_torch_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_patched_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module, \n\u001b[0m\u001b[1;32m     13\u001b[0m                                  weights_only=False, mmap=mmap, **kwargs)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2725925807.py\u001b[0m in \u001b[0;36m_patched_load\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_original_torch_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_patched_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module, \n\u001b[0m\u001b[1;32m     10\u001b[0m                                  weights_only=False, mmap=mmap, **kwargs)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "... last 1 frames repeated, from the frame below ...\n",
            "\u001b[0;32m/tmp/ipython-input-2725925807.py\u001b[0m in \u001b[0;36m_patched_load\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **kwargs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_original_torch_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_patched_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     return _original_torch_load(f, map_location=map_location, pickle_module=pickle_module, \n\u001b[0m\u001b[1;32m     10\u001b[0m                                  weights_only=False, mmap=mmap, **kwargs)\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install onnxscript and export\n",
        "!pip install onnxscript -q\n",
        "\n",
        "print(\"‚úÖ onnxscript installed!\")\n",
        "\n",
        "# Export to ONNX\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('cattle_breed_recognition/cattle_detector/weights/best.pt')\n",
        "model.export(format='onnx', imgsz=320, simplify=True)\n",
        "\n",
        "print(\"‚úÖ ONNX exported!\")\n"
      ],
      "metadata": {
        "id": "hBTzoZqWoeo6",
        "outputId": "6e137dab-f84b-4911-d438-3170dde134f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/689.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m286.7/689.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/159.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ onnxscript installed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.12.12 torch-2.10.0+cpu CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'cattle_breed_recognition/cattle_detector/weights/best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 5, 2100) (5.9 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.1 opset 10...\n",
            "W0223 13:13:23.937000 25849 torch/onnx/_internal/exporter/_compat.py:125] Setting ONNX exporter to use operator set version 18 because the requested opset_version 10 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n",
            "W0223 13:13:24.626000 25849 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0223 13:13:24.628000 25849 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0223 13:13:24.629000 25849 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0223 13:13:24.632000 25849 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 10).\n",
            "WARNING:onnxscript.version_converter:Failed to convert the model to the target version 10 using the ONNX C API. The model was not modified\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 120, in call\n",
            "    converted_proto = _c_api_utils.call_onnx_api(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
            "    result = func(proto)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 115, in _partial_convert_version\n",
            "    return onnx.version_converter.convert_version(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/version_converter.py\", line 39, in convert_version\n",
            "    converted_model_str = C.convert_version(model_str, target_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: /github/workspace/onnx/version_converter/BaseConverter.h:65: adapter_lookup: Assertion `false` failed: No Adapter To Version $17 for Resize\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m simplifying with onnxsim 0.5.0...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applied 1 of general pattern rewrite rules.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 4.8s, saved as 'cattle_breed_recognition/cattle_detector/weights/best.onnx' (11.5 MB)\n",
            "\n",
            "Export complete (6.4s)\n",
            "Results saved to \u001b[1m/content/cattle_breed_recognition/cattle_detector/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=cattle_breed_recognition/cattle_detector/weights/best.onnx imgsz=320  \n",
            "Validate:        yolo val task=detect model=cattle_breed_recognition/cattle_detector/weights/best.onnx imgsz=320 data=/content/detection_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ONNX exported!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPLETE TRAINING SCRIPT - Run after restart\n",
        "# Do NOT run any previous cells\n",
        "\n",
        "# Step 1: Install ultralytics\n",
        "!pip install ultralytics -q\n",
        "print(\"‚úÖ Ultralytics installed!\")\n",
        "\n",
        "# Step 2: Apply PyTorch fix (ONLY ONCE)\n",
        "import torch\n",
        "import functools\n",
        "\n",
        "# Get the ORIGINAL torch.load before any modifications\n",
        "_torch_load = torch.load\n",
        "\n",
        "# Create wrapper\n",
        "@functools.wraps(_torch_load)\n",
        "def _patched_torch_load(f, map_location=None, pickle_module=None, *, weights_only=None, mmap=None, **kwargs):\n",
        "    return _torch_load(f, map_location=map_location, pickle_module=pickle_module,\n",
        "                       weights_only=False, mmap=mmap, **kwargs)\n",
        "\n",
        "# Apply patch ONCE\n",
        "torch.load = _patched_torch_load\n",
        "print(\"‚úÖ PyTorch patch applied!\")\n",
        "\n",
        "# Step 3: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Google Drive mounted!\")\n",
        "\n",
        "# Step 4: Load ZIP files from Drive backup\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import subprocess\n",
        "import glob\n",
        "import random\n",
        "\n",
        "backup_folder = '/content/drive/MyDrive/cattle_breed_recognition/dataset_backup'\n",
        "print(\"\\nüìÇ Loading ZIP files from Google Drive...\")\n",
        "\n",
        "if os.path.exists(backup_folder):\n",
        "    for f in os.listdir(backup_folder):\n",
        "        if f.endswith('.zip'):\n",
        "            src = os.path.join(backup_folder, f)\n",
        "            dst = f'/content/{f}'\n",
        "            if not os.path.exists(dst):\n",
        "                size_mb = os.path.getsize(src) / (1024 * 1024)\n",
        "                print(f\"   Loading: {f} ({size_mb:.1f} MB)\")\n",
        "                shutil.copy2(src, dst)\n",
        "            else:\n",
        "                print(f\"   Already exists: {f}\")\n",
        "\n",
        "# Step 5: Create dataset\n",
        "DATASET_PATH = '/content/detection_dataset'\n",
        "\n",
        "# Clean and create directories\n",
        "if os.path.exists('/content/temp_images'):\n",
        "    shutil.rmtree('/content/temp_images')\n",
        "os.makedirs('/content/temp_images', exist_ok=True)\n",
        "\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    os.makedirs(f'{DATASET_PATH}/{split}/images', exist_ok=True)\n",
        "    os.makedirs(f'{DATASET_PATH}/{split}/labels', exist_ok=True)\n",
        "\n",
        "print(\"\\nüì¶ Extracting ZIP files...\")\n",
        "zip_files = ['/content/cow breed-1.zip', '/content/cow breed-2.zip', '/content/cow breed-3.zip']\n",
        "\n",
        "for zip_path in zip_files:\n",
        "    if os.path.exists(zip_path):\n",
        "        print(f\"   Extracting: {os.path.basename(zip_path)}\")\n",
        "        subprocess.run(['unzip', '-o', '-q', zip_path, '-d', '/content/temp_images'], capture_output=True)\n",
        "\n",
        "# Count images\n",
        "all_images = []\n",
        "for ext in ['*.jpg', '*.jpeg', '*.png', '*.webp', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "    all_images.extend(glob.glob(f'/content/temp_images/**/{ext}', recursive=True))\n",
        "\n",
        "print(f\"   Found {len(all_images)} images\")\n",
        "\n",
        "# Split and copy\n",
        "if len(all_images) > 0:\n",
        "    random.shuffle(all_images)\n",
        "    n = len(all_images)\n",
        "\n",
        "    for i, img_path in enumerate(all_images[:int(n*0.7)]):\n",
        "        ext = os.path.splitext(img_path)[1]\n",
        "        shutil.copy(img_path, f'{DATASET_PATH}/train/images/image_{i}{ext}')\n",
        "        with open(f'{DATASET_PATH}/train/labels/image_{i}.txt', 'w') as f:\n",
        "            f.write('0 0.5 0.5 0.8 0.8\\n')\n",
        "\n",
        "    for i, img_path in enumerate(all_images[int(n*0.7):int(n*0.85)]):\n",
        "        ext = os.path.splitext(img_path)[1]\n",
        "        shutil.copy(img_path, f'{DATASET_PATH}/valid/images/image_{i}{ext}')\n",
        "        with open(f'{DATASET_PATH}/valid/labels/image_{i}.txt', 'w') as f:\n",
        "            f.write('0 0.5 0.5 0.8 0.8\\n')\n",
        "\n",
        "    for i, img_path in enumerate(all_images[int(n*0.85):]):\n",
        "        ext = os.path.splitext(img_path)[1]\n",
        "        shutil.copy(img_path, f'{DATASET_PATH}/test/images/image_{i}{ext}')\n",
        "        with open(f'{DATASET_PATH}/test/labels/image_{i}.txt', 'w') as f:\n",
        "            f.write('0 0.5 0.5 0.8 0.8\\n')\n",
        "\n",
        "    shutil.rmtree('/content/temp_images', ignore_errors=True)\n",
        "    print(f\"‚úÖ Dataset: {int(n*0.7)} train, {int(n*0.15)} valid, {n-int(n*0.85)} test\")\n",
        "\n",
        "# Step 6: Create data.yaml\n",
        "yaml_content = f\"\"\"names:\n",
        "- cattle\n",
        "nc: 1\n",
        "path: {DATASET_PATH}\n",
        "test: test/images\n",
        "train: train/images\n",
        "val: valid/images\n",
        "\"\"\"\n",
        "with open(f'{DATASET_PATH}/data.yaml', 'w') as f:\n",
        "    f.write(yaml_content)\n",
        "print(\"‚úÖ data.yaml created!\")\n",
        "\n",
        "# Step 7: Train\n",
        "print(\"\\nüöÄ Training YOLOv8-Nano...\")\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "results = model.train(\n",
        "    data=f'{DATASET_PATH}/data.yaml',\n",
        "    epochs=20,\n",
        "    imgsz=320,\n",
        "    batch=16,\n",
        "    name='cattle_detector',\n",
        "    project='cattle_breed_recognition',\n",
        "    device='cpu',\n",
        "    patience=5,\n",
        "    save=True\n",
        ")\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "\n",
        "# Step 8: Export and Save\n",
        "model.export(format='onnx', imgsz=320, simplify=True)\n",
        "print(\"‚úÖ ONNX exported!\")\n",
        "\n",
        "save_path = '/content/drive/MyDrive/cattle_breed_recognition/models'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "!cp cattle_breed_recognition/cattle_detector/weights/best.pt {save_path}/cattle_detector.pt\n",
        "!cp cattle_breed_recognition/cattle_detector/weights/best.onnx {save_path}/cattle_detector.onnx\n",
        "print(\"‚úÖ Model saved to Google Drive!\")\n",
        "\n",
        "print(\"\\nüéâ ALL DONE!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5P45EbbqQovb",
        "outputId": "76d16055-e628-413f-965e-d44edd7f2b8a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Ultralytics installed!\n",
            "‚úÖ PyTorch patch applied!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Google Drive mounted!\n",
            "\n",
            "üìÇ Loading ZIP files from Google Drive...\n",
            "   Already exists: cow breed-3.zip\n",
            "   Already exists: cow breed-2.zip\n",
            "   Already exists: cow breed-1.zip\n",
            "\n",
            "üì¶ Extracting ZIP files...\n",
            "   Extracting: cow breed-1.zip\n",
            "   Extracting: cow breed-2.zip\n",
            "   Extracting: cow breed-3.zip\n",
            "   Found 2201 images\n",
            "‚úÖ Dataset: 1540 train, 330 valid, 331 test\n",
            "‚úÖ data.yaml created!\n",
            "\n",
            "üöÄ Training YOLOv8-Nano...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "New https://pypi.org/project/ultralytics/8.4.14 available üòÉ Update with 'pip install -U ultralytics'\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.12.12 torch-2.10.0+cpu CPU (AMD EPYC 7B12)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/detection_dataset/data.yaml, epochs=20, patience=5, batch=16, imgsz=320, save=True, save_period=-1, cache=False, device=cpu, workers=8, project=cattle_breed_recognition, name=cattle_detector, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=cattle_breed_recognition/cattle_detector\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 755k/755k [00:00<00:00, 15.9MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir cattle_breed_recognition/cattle_detector', view at http://localhost:6006/\n",
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using W&B in offline mode.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.25.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260223_113356-j2cb6cnm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py:238: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = amp.GradScaler(enabled=self.amp)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/detection_dataset/train/labels... 1875 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1875/1875 [00:02<00:00, 721.63it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/detection_dataset/train/labels.cache\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:805: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/detection_dataset/valid/labels... 408 images, 0 backgrounds, 0 corrupt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 408/408 [00:00<00:00, 1849.89it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/detection_dataset/valid/labels.cache\n",
            "Plotting labels to cattle_breed_recognition/cattle_detector/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 320 train, 320 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mcattle_breed_recognition/cattle_detector\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20         0G     0.8962      1.037      1.167          9        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:26<00:00,  2.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:20<00:00,  1.60s/it]\n",
            "                   all        408        408      0.979      0.993      0.995      0.818\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20         0G     0.6553     0.5696      1.036          6        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:30<00:00,  2.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.51s/it]\n",
            "                   all        408        408      0.993      0.996      0.995      0.852\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20         0G     0.6264     0.5095      1.015          7        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:22<00:00,  2.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.47s/it]\n",
            "                   all        408        408      0.998          1      0.995      0.864\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20         0G     0.5891     0.4599      1.001          7        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:24<00:00,  2.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.51s/it]\n",
            "                   all        408        408      0.993          1      0.994      0.789\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20         0G      0.566     0.4399     0.9979         10        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:20<00:00,  2.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.51s/it]\n",
            "                   all        408        408      0.998      0.991      0.995      0.938\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20         0G     0.5284     0.3972     0.9765          7        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:25<00:00,  2.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:20<00:00,  1.58s/it]\n",
            "                   all        408        408      0.998      0.996      0.995      0.896\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20         0G     0.5019     0.3806     0.9714         10        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:25<00:00,  2.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.53s/it]\n",
            "                   all        408        408      0.997          1      0.995      0.963\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20         0G     0.4729     0.3536     0.9659         11        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:23<00:00,  2.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.49s/it]\n",
            "                   all        408        408          1      0.999      0.995      0.947\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20         0G     0.4581     0.3415     0.9572          9        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:29<00:00,  2.29s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.53s/it]\n",
            "                   all        408        408          1      0.997      0.995      0.926\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20         0G     0.4396     0.3239      0.952          6        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:24<00:00,  2.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.47s/it]\n",
            "                   all        408        408          1          1      0.995      0.978\n",
            "Closing dataloader mosaic\n",
            "/usr/local/lib/python3.12/dist-packages/ultralytics/data/augment.py:805: UserWarning: Argument(s) 'quality_lower' are not valid for transform ImageCompression\n",
            "  A.ImageCompression(quality_lower=75, p=0.0)]  # transforms\n",
            "/usr/local/lib/python3.12/dist-packages/albumentations/core/composition.py:331: UserWarning: Got processor for bboxes, but no transform to process it.\n",
            "  self._set_keys()\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  super().__init__(loader)\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20         0G     0.4192     0.3025     0.9437          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:21<00:00,  2.21s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.49s/it]\n",
            "                   all        408        408          1          1      0.995      0.951\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20         0G     0.3862     0.2579     0.9247          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:23<00:00,  2.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:21<00:00,  1.66s/it]\n",
            "                   all        408        408          1          1      0.995      0.988\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20         0G     0.3753     0.2378      0.925          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:22<00:00,  2.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:20<00:00,  1.61s/it]\n",
            "                   all        408        408          1          1      0.995       0.99\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20         0G     0.3565     0.2226      0.918          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:23<00:00,  2.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:20<00:00,  1.59s/it]\n",
            "                   all        408        408      0.998          1      0.995      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20         0G     0.3403     0.2067     0.9167          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:21<00:00,  2.22s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:20<00:00,  1.59s/it]\n",
            "                   all        408        408          1          1      0.995      0.995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20         0G      0.314     0.1979     0.8999          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:24<00:00,  2.24s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.52s/it]\n",
            "                   all        408        408          1          1      0.995      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20         0G     0.2968     0.1827     0.8949          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:23<00:00,  2.23s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.48s/it]\n",
            "                   all        408        408          1          1      0.995      0.993\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20         0G     0.2925     0.1785     0.9024          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:28<00:00,  2.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.51s/it]\n",
            "                   all        408        408          1          1      0.995      0.995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20         0G     0.2799     0.1707     0.8884          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:26<00:00,  2.26s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.52s/it]\n",
            "                   all        408        408          1          1      0.995      0.995\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20         0G     0.2757     0.1637     0.8912          3        320: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 118/118 [04:27<00:00,  2.27s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:19<00:00,  1.53s/it]\n",
            "                   all        408        408          1          1      0.995      0.995\n",
            "Stopping training early as no improvement observed in last 5 epochs. Best results observed at epoch 15, best model saved as best.pt.\n",
            "To update EarlyStopping(patience=5) pass a new patience value, i.e. `patience=300` or use `patience=0` to disable EarlyStopping.\n",
            "\n",
            "20 epochs completed in 1.587 hours.\n",
            "Optimizer stripped from cattle_breed_recognition/cattle_detector/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from cattle_breed_recognition/cattle_detector/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating cattle_breed_recognition/cattle_detector/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.12.12 torch-2.10.0+cpu CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:18<00:00,  1.46s/it]\n",
            "                   all        408        408          1          1      0.995      0.995\n",
            "Speed: 0.3ms preprocess, 37.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
            "Results saved to \u001b[1mcattle_breed_recognition/cattle_detector\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>lr/pg1</td><td>‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>lr/pg2</td><td>‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>metrics/mAP50(B)</td><td>‚ñÑ‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>metrics/mAP50-95(B)</td><td>‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>metrics/precision(B)</td><td>‚ñÅ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>metrics/recall(B)</td><td>‚ñÇ‚ñÖ‚ñà‚ñà‚ñÅ‚ñÖ‚ñà‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>model/GFLOPs</td><td>‚ñÅ</td></tr><tr><td>model/parameters</td><td>‚ñÅ</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>‚ñÅ</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr/pg0</td><td>0.00022</td></tr><tr><td>lr/pg1</td><td>0.00022</td></tr><tr><td>lr/pg2</td><td>0.00022</td></tr><tr><td>metrics/mAP50(B)</td><td>0.995</td></tr><tr><td>metrics/mAP50-95(B)</td><td>0.99496</td></tr><tr><td>metrics/precision(B)</td><td>0.99954</td></tr><tr><td>metrics/recall(B)</td><td>1</td></tr><tr><td>model/GFLOPs</td><td>8.194</td></tr><tr><td>model/parameters</td><td>3011043</td></tr><tr><td>model/speed_PyTorch(ms)</td><td>40.507</td></tr><tr><td>+6</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "You can sync this run to the cloud by running:<br><code>wandb sync /content/wandb/offline-run-20260223_113356-j2cb6cnm<code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/offline-run-20260223_113356-j2cb6cnm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Training complete!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.12.12 torch-2.10.0+cpu CPU (AMD EPYC 7B12)\n",
            "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'cattle_breed_recognition/cattle_detector/weights/best.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) (1, 5, 2100) (5.9 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['onnx>=1.12.0', 'onnxsim>=0.4.33', 'onnxruntime'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.20.1-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Collecting onnxsim>=0.4.33\n",
            "  Downloading onnxsim-0.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (5.29.6)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.12.0) (0.5.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from onnxsim>=0.4.33) (13.9.4)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.12.19)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (26.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim>=0.4.33) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim>=0.4.33) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim>=0.4.33) (0.1.2)\n",
            "Downloading onnx-1.20.1-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.5 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17.5/17.5 MB 232.2 MB/s eta 0:00:00\n",
            "Downloading onnxsim-0.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.2 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.2/2.2 MB 304.1 MB/s eta 0:00:00\n",
            "Downloading onnxruntime-1.24.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.1 MB)\n",
            "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 17.1/17.1 MB 198.3 MB/s eta 0:00:00\n",
            "Installing collected packages: onnxruntime, onnx, onnxsim\n",
            "Successfully installed onnx-1.20.1 onnxruntime-1.24.2 onnxsim-0.5.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 10.2s, installed 3 packages: ['onnx>=1.12.0', 'onnxsim>=0.4.33', 'onnxruntime']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.1 opset 10...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export failure ‚ùå 10.6s: No module named 'onnxscript'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'onnxscript'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3589512583.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;31m# Step 8: Export and Save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'onnx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimplify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ ONNX exported!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mcustom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'imgsz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'imgsz'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# method defaults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcustom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mode'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'export'\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# highest priority args on the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mExporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_callbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monnx\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# OpenVINO requires ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# OpenVINO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_openvino\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36mouter_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{prefix} export failure ‚ùå {dt.t:.1f}s: {e}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mouter_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36mouter_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mProfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{prefix} export success ‚úÖ {dt.t:.1f}s, saved as '{f}' ({file_size(f):.1f} MB)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ultralytics/engine/exporter.py\u001b[0m in \u001b[0;36mexport_onnx\u001b[0;34m(self, prefix)\u001b[0m\n\u001b[1;32m    331\u001b[0m                 \u001b[0mdynamic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output0'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'anchors'\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# shape(1, 84, 8400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         torch.onnx.export(\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dynamic=True only compatible with cpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdynamic\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, verbose, input_names, output_names, opset_version, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, export_params, keep_initializers_as_inputs, dynamic_axes, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \"\"\"\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdynamo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportedProgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_constants\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0monnx_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_import\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnxscript_apis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnxscript_ir\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from torch.onnx._internal.exporter import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0m_constants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0m_core\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monnxscript\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxscript'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to Google Drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "save_path = '/content/drive/MyDrive/cattle_breed_recognition/models'\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Copy files\n",
        "shutil.copy('cattle_breed_recognition/cattle_detector/weights/best.pt', f'{save_path}/cattle_detector.pt')\n",
        "shutil.copy('cattle_breed_recognition/cattle_detector/weights/best.onnx', f'{save_path}/cattle_detector.onnx')\n",
        "\n",
        "print(\"‚úÖ Model saved to Google Drive!\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\nüìÅ Saved files:\")\n",
        "for f in os.listdir(save_path):\n",
        "    fpath = os.path.join(save_path, f)\n",
        "    if os.path.isfile(fpath):\n",
        "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
        "        print(f\"   ‚úÖ {f} ({size_mb:.1f} MB)\")\n",
        "\n",
        "print(\"\\nüéâ ALL DONE! Model trained and saved!\")\n"
      ],
      "metadata": {
        "id": "9JarMYCMpNSm",
        "outputId": "6ad122a7-468a-43e0-f7f2-584a501ac844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved to Google Drive!\n",
            "\n",
            "üìÅ Saved files:\n",
            "   ‚úÖ cattle_detector.pt (5.9 MB)\n",
            "   ‚úÖ cattle_detector.onnx (11.5 MB)\n",
            "\n",
            "üéâ ALL DONE! Model trained and saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feZXxitD4Xaq"
      },
      "source": [
        "## 1. Setup Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f-JVEDo4Xar"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Install YOLOv8\n",
        "!pip install ultralytics==8.0.196 -q\n",
        "!pip install roboflow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYNrV6Qu4Xas"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "import json\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for data storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "BASE_PATH = '/content/drive/MyDrive/cattle_breed_recognition'\n",
        "DATA_PATH = f'{BASE_PATH}/data'\n",
        "MODELS_PATH = f'{BASE_PATH}/models'\n",
        "os.makedirs(DATA_PATH, exist_ok=True)\n",
        "os.makedirs(MODELS_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Base Path: {BASE_PATH}\")\n",
        "print(f\"Data Path: {DATA_PATH}\")\n",
        "print(f\"Models Path: {MODELS_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu3RHhRy4Xat"
      },
      "source": [
        "## 2. Prepare Dataset\n",
        "\n",
        "### Option A: Download from Roboflow (Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSmUb-E24Xat"
      },
      "outputs": [],
      "source": [
        "# Option A: Download cattle detection dataset from Roboflow\n",
        "# You can find cattle detection datasets at: https://universe.roboflow.com/\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"YOUR_API_KEY\")\n",
        "project = rf.workspace(\"workspace-name\").project(\"cattle-detection\")\n",
        "dataset = project.version(1).download(\"yolov8\")\n",
        "\n",
        "# The dataset will be downloaded in YOLOv8 format\n",
        "DATASET_PATH = dataset.location\n",
        "print(f\"Dataset downloaded to: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS985VLP4Xau"
      },
      "source": [
        "### Option B: Use Local Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ri-FoNB4Xau"
      },
      "outputs": [],
      "source": [
        "# Option B: Upload your own dataset\n",
        "# Dataset structure should be:\n",
        "# dataset/\n",
        "# ‚îú‚îÄ‚îÄ data.yaml\n",
        "# ‚îú‚îÄ‚îÄ train/\n",
        "# ‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
        "# ‚îÇ   ‚îî‚îÄ‚îÄ labels/\n",
        "# ‚îú‚îÄ‚îÄ valid/\n",
        "# ‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
        "# ‚îÇ   ‚îî‚îÄ‚îÄ labels/\n",
        "# ‚îî‚îÄ‚îÄ test/\n",
        "#     ‚îú‚îÄ‚îÄ images/\n",
        "#     ‚îî‚îÄ‚îÄ labels/\n",
        "\n",
        "# Upload dataset.zip to Colab and extract\n",
        "# !unzip dataset.zip -d {DATA_PATH}\n",
        "\n",
        "# DATASET_PATH = f'{DATA_PATH}/dataset'\n",
        "# print(f\"Dataset path: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5ZVUgld4Xav"
      },
      "source": [
        "### Option C: Create Dataset from Kaggle Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjXjUzno4Xav"
      },
      "outputs": [],
      "source": [
        "# Option C: Create detection dataset from Kaggle breed images\n",
        "# This creates bounding boxes around the entire image (simple approach)\n",
        "\n",
        "def create_detection_dataset_from_classification(source_dir, output_dir, class_name='cattle'):\n",
        "    \"\"\"\n",
        "    Create YOLO format detection dataset from classification images.\n",
        "    Uses entire image as bounding box.\n",
        "\n",
        "    Args:\n",
        "        source_dir: Directory with class folders (classification format)\n",
        "        output_dir: Output directory for YOLO format\n",
        "        class_name: Class name for detection (cattle/buffalo)\n",
        "    \"\"\"\n",
        "    import random\n",
        "\n",
        "    # Create output directories\n",
        "    for split in ['train', 'valid', 'test']:\n",
        "        os.makedirs(f'{output_dir}/{split}/images', exist_ok=True)\n",
        "        os.makedirs(f'{output_dir}/{split}/labels', exist_ok=True)\n",
        "\n",
        "    # Collect all images\n",
        "    all_images = []\n",
        "    for breed_folder in os.listdir(source_dir):\n",
        "        breed_path = os.path.join(source_dir, breed_folder)\n",
        "        if os.path.isdir(breed_path):\n",
        "            for img_name in os.listdir(breed_path):\n",
        "                if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                    all_images.append({\n",
        "                        'path': os.path.join(breed_path, img_name),\n",
        "                        'name': f\"{breed_folder}_{img_name}\"\n",
        "                    })\n",
        "\n",
        "    print(f\"Total images found: {len(all_images)}\")\n",
        "\n",
        "    # Shuffle and split\n",
        "    random.shuffle(all_images)\n",
        "    n = len(all_images)\n",
        "    train_split = int(0.7 * n)\n",
        "    val_split = int(0.85 * n)\n",
        "\n",
        "    splits = {\n",
        "        'train': all_images[:train_split],\n",
        "        'valid': all_images[train_split:val_split],\n",
        "        'test': all_images[val_split:]\n",
        "    }\n",
        "\n",
        "    # Process each split\n",
        "    for split_name, images in splits.items():\n",
        "        print(f\"Processing {split_name}: {len(images)} images\")\n",
        "\n",
        "        for img_info in images:\n",
        "            # Copy image\n",
        "            img_path = img_info['path']\n",
        "            img_name = img_info['name']\n",
        "            dest_img = f'{output_dir}/{split_name}/images/{img_name}'\n",
        "            shutil.copy(img_path, dest_img)\n",
        "\n",
        "            # Get image dimensions\n",
        "            img = cv2.imread(img_path)\n",
        "            h, w = img.shape[:2]\n",
        "\n",
        "            # Create YOLO format label (entire image as bbox)\n",
        "            # YOLO format: class x_center y_center width height (normalized 0-1)\n",
        "            # class 0 = cattle\n",
        "            label_name = img_name.rsplit('.', 1)[0] + '.txt'\n",
        "            label_path = f'{output_dir}/{split_name}/labels/{label_name}'\n",
        "\n",
        "            # Entire image as bounding box\n",
        "            x_center = 0.5\n",
        "            y_center = 0.5\n",
        "            width = 1.0\n",
        "            height = 1.0\n",
        "\n",
        "            with open(label_path, 'w') as f:\n",
        "                f.write(f\"0 {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "    print(f\"Dataset created at: {output_dir}\")\n",
        "    return output_dir\n",
        "\n",
        "# Uncomment to use:\n",
        "# SOURCE_DATA = f'{DATA_PATH}/raw/kaggle_cattle_images'\n",
        "# OUTPUT_DATA = f'{DATA_PATH}/detection_dataset'\n",
        "# create_detection_dataset_from_classification(SOURCE_DATA, OUTPUT_DATA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OriPyEW4Xav"
      },
      "source": [
        "## 3. Create data.yaml Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPX48jrd4Xaw"
      },
      "outputs": [],
      "source": [
        "# Create data.yaml for YOLOv8\n",
        "\n",
        "def create_data_yaml(dataset_path, output_path):\n",
        "    \"\"\"\n",
        "    Create data.yaml configuration file for YOLOv8 training.\n",
        "    \"\"\"\n",
        "    data_config = {\n",
        "        'path': dataset_path,\n",
        "        'train': 'train/images',\n",
        "        'val': 'valid/images',\n",
        "        'test': 'test/images',\n",
        "        'nc': 1,  # Number of classes (1 = cattle/buffalo)\n",
        "        'names': ['cattle']  # Class names\n",
        "    }\n",
        "\n",
        "    with open(output_path, 'w') as f:\n",
        "        yaml.dump(data_config, f, default_flow_style=False)\n",
        "\n",
        "    print(f\"Created data.yaml at: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "# Create data.yaml\n",
        "DATASET_PATH = f'{DATA_PATH}/detection_dataset'  # Update this path\n",
        "YAML_PATH = create_data_yaml(DATASET_PATH, f'{DATASET_PATH}/data.yaml')\n",
        "\n",
        "# Display config\n",
        "with open(YAML_PATH, 'r') as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QH_Kv3L4Xaw"
      },
      "source": [
        "## 4. Train YOLOv8-Nano Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GX8uvrsb4Xaw"
      },
      "outputs": [],
      "source": [
        "# Load YOLOv8-Nano model (pretrained on COCO)\n",
        "model = YOLO('yolov8n.pt')  # Nano model\n",
        "\n",
        "# Training parameters\n",
        "training_params = {\n",
        "    'data': YAML_PATH,\n",
        "    'epochs': 100,              # Number of training epochs\n",
        "    'imgsz': 416,               # Image size (smaller for mobile)\n",
        "    'batch': 32,                # Batch size\n",
        "    'name': 'cattle_detector',  # Experiment name\n",
        "    'project': MODELS_PATH,     # Save directory\n",
        "    'device': 0,                # GPU device\n",
        "    'patience': 20,             # Early stopping patience\n",
        "    'save': True,               # Save checkpoints\n",
        "    'save_period': 10,          # Save every N epochs\n",
        "    'workers': 4,               # Data loading workers\n",
        "    'pretrained': True,         # Use pretrained weights\n",
        "    'optimizer': 'auto',        # Optimizer (auto selects AdamW)\n",
        "    'lr0': 0.01,                # Initial learning rate\n",
        "    'lrf': 0.01,                # Final learning rate\n",
        "    'momentum': 0.937,          # SGD momentum\n",
        "    'weight_decay': 0.0005,     # Weight decay\n",
        "    'warmup_epochs': 3,         # Warmup epochs\n",
        "    'warmup_momentum': 0.8,     # Warmup momentum\n",
        "    'warmup_bias_lr': 0.1,      # Warmup bias learning rate\n",
        "    'box': 7.5,                 # Box loss gain\n",
        "    'cls': 0.5,                 # Classification loss gain\n",
        "    'dfl': 1.5,                 # Distribution focal loss gain\n",
        "    'pose': 12.0,               # Pose loss gain\n",
        "    'kobj': 1.0,                # Keypoint objectness loss gain\n",
        "    'label_smoothing': 0.0,     # Label smoothing\n",
        "    'nbs': 64,                  # Nominal batch size\n",
        "    'hsv_h': 0.015,             # HSV-Hue augmentation\n",
        "    'hsv_s': 0.7,               # HSV-Saturation augmentation\n",
        "    'hsv_v': 0.4,               # HSV-Value augmentation\n",
        "    'degrees': 15.0,            # Rotation augmentation (+/- deg)\n",
        "    'translate': 0.1,           # Translation augmentation (+/- fraction)\n",
        "    'scale': 0.5,               # Scaling augmentation (+/- gain)\n",
        "    'shear': 0.0,               # Shear augmentation (+/- deg)\n",
        "    'perspective': 0.0,         # Perspective augmentation (+/- fraction)\n",
        "    'flipud': 0.0,              # Flip up-down probability\n",
        "    'fliplr': 0.5,              # Flip left-right probability\n",
        "    'mosaic': 1.0,              # Mosaic augmentation probability\n",
        "    'mixup': 0.0,               # Mixup augmentation probability\n",
        "    'copy_paste': 0.0,          # Copy-paste augmentation probability\n",
        "    'auto_augment': 'randaugment',  # Auto augmentation policy\n",
        "    'erasing': 0.4,             # Random erasing probability\n",
        "    'crop_fraction': 1.0,       # Image crop fraction\n",
        "}\n",
        "\n",
        "print(\"Starting YOLOv8-Nano training...\")\n",
        "print(f\"Dataset: {YAML_PATH}\")\n",
        "print(f\"Image size: 416x416\")\n",
        "print(f\"Epochs: 100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIWfsg8N4Xaw"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "results = model.train(**training_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5mhIhmc4Xaw"
      },
      "source": [
        "## 5. Evaluate Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9zWan0J4Xaw"
      },
      "outputs": [],
      "source": [
        "# Validate the model\n",
        "metrics = model.val()\n",
        "\n",
        "print(\"\\n=== Validation Metrics ===\")\n",
        "print(f\"mAP@50: {metrics.box.map50:.4f}\")\n",
        "print(f\"mAP@50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
        "print(f\"Recall: {metrics.box.mr:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnYHiy1N4Xax"
      },
      "outputs": [],
      "source": [
        "# Plot training results\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Display results plot\n",
        "results_path = f'{MODELS_PATH}/cattle_detector'\n",
        "if os.path.exists(f'{results_path}/results.png'):\n",
        "    display(Image(filename=f'{results_path}/results.png', width=800))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjgOhovg4Xax"
      },
      "outputs": [],
      "source": [
        "# Test on sample images\n",
        "test_images_path = f'{DATASET_PATH}/test/images'\n",
        "test_images = os.listdir(test_images_path)[:5]\n",
        "\n",
        "for img_name in test_images:\n",
        "    img_path = os.path.join(test_images_path, img_name)\n",
        "    results = model.predict(img_path, save=True, conf=0.25)\n",
        "\n",
        "    # Display result\n",
        "    result_img = f'{results[0].save_dir}/{img_name}'\n",
        "    if os.path.exists(result_img):\n",
        "        display(Image(filename=result_img, width=400))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcTJQi-X4Xax"
      },
      "source": [
        "## 6. Export to TFLite with INT8 Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du5Vf5i84Xax"
      },
      "outputs": [],
      "source": [
        "# Export to TFLite format\n",
        "# YOLOv8 supports direct TFLite export\n",
        "\n",
        "# Load best model\n",
        "best_model = YOLO(f'{MODELS_PATH}/cattle_detector/weights/best.pt')\n",
        "\n",
        "# Export to TFLite\n",
        "best_model.export(\n",
        "    format='tflite',\n",
        "    imgsz=416,\n",
        "    int8=True,           # INT8 quantization\n",
        "    data=YAML_PATH,      # Dataset for calibration\n",
        "    batch=1,\n",
        "    optimize=True,       # Optimize for mobile\n",
        "    simplify=True,       # Simplify model\n",
        "    opset=12,            # ONNX opset version\n",
        "    workspace=4,         # TensorRT workspace size (GB)\n",
        ")\n",
        "\n",
        "print(\"Model exported to TFLite with INT8 quantization!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfJ0cKs94Xax"
      },
      "outputs": [],
      "source": [
        "# Check exported model size\n",
        "import glob\n",
        "\n",
        "tflite_files = glob.glob(f'{MODELS_PATH}/cattle_detector/weights/*.tflite')\n",
        "for tflite_file in tflite_files:\n",
        "    size_mb = os.path.getsize(tflite_file) / (1024 * 1024)\n",
        "    print(f\"{os.path.basename(tflite_file)}: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2GQ6SZ84Xax"
      },
      "source": [
        "## 7. Test TFLite Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SRt1nCA4Xay"
      },
      "outputs": [],
      "source": [
        "# Test TFLite model inference\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def test_tflite_model(tflite_path, test_image_path):\n",
        "    \"\"\"\n",
        "    Test TFLite model inference on a single image.\n",
        "    \"\"\"\n",
        "    # Load TFLite model\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input/output details\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(f\"Input shape: {input_details[0]['shape']}\")\n",
        "    print(f\"Input dtype: {input_details[0]['dtype']}\")\n",
        "    print(f\"Output shape: {output_details[0]['shape']}\")\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = cv2.imread(test_image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = cv2.resize(img, (416, 416))\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    # Run inference\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], img)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "    inference_time = (time.time() - start_time) * 1000\n",
        "    print(f\"\\nInference time: {inference_time:.2f} ms\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test on sample image\n",
        "tflite_path = glob.glob(f'{MODELS_PATH}/cattle_detector/weights/*_int8.tflite')[0]\n",
        "test_image = os.path.join(test_images_path, test_images[0])\n",
        "\n",
        "print(f\"Testing TFLite model: {tflite_path}\")\n",
        "print(f\"Test image: {test_image}\")\n",
        "\n",
        "output = test_tflite_model(tflite_path, test_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UwZOO_14Xay"
      },
      "source": [
        "## 8. Save Final Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb1ZS8SX4Xay"
      },
      "outputs": [],
      "source": [
        "# Copy final models to output directory\n",
        "OUTPUT_DIR = f'{MODELS_PATH}/final'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Copy best PyTorch model\n",
        "shutil.copy(\n",
        "    f'{MODELS_PATH}/cattle_detector/weights/best.pt',\n",
        "    f'{OUTPUT_DIR}/yolov8_nano_cattle_detector.pt'\n",
        ")\n",
        "\n",
        "# Copy TFLite model\n",
        "for tflite_file in tflite_files:\n",
        "    shutil.copy(tflite_file, OUTPUT_DIR)\n",
        "\n",
        "print(f\"Final models saved to: {OUTPUT_DIR}\")\n",
        "print(\"\\nFiles:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    size_mb = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / (1024 * 1024)\n",
        "    print(f\"  {f}: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijm3T76E4Xay"
      },
      "source": [
        "## 9. Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXnaY9GC4Xay"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"YOLOv8-Nano Detection Model Training Complete!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nModel: YOLOv8-Nano\")\n",
        "print(f\"Task: Cattle/Buffalo Detection\")\n",
        "print(f\"Input Size: 416x416\")\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"  mAP@50: {metrics.box.map50:.4f}\")\n",
        "print(f\"  mAP@50-95: {metrics.box.map:.4f}\")\n",
        "print(f\"  Precision: {metrics.box.mp:.4f}\")\n",
        "print(f\"  Recall: {metrics.box.mr:.4f}\")\n",
        "print(f\"\\nModel Files:\")\n",
        "print(f\"  PyTorch: yolov8_nano_cattle_detector.pt\")\n",
        "print(f\"  TFLite INT8: *_int8.tflite (~5 MB)\")\n",
        "print(f\"\\nReady for Stage 2: Breed Classification with EfficientNet-B0\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}