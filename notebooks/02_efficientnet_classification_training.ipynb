{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientNet-B0 Classification Model Training\n",
    "## Cattle Breed Recognition System - Stage 2: Breed Classification\n",
    "\n",
    "This notebook trains an EfficientNet-B0 model for classifying 35 Indian cattle and buffalo breeds.\n",
    "\n",
    "**Model Specifications:**\n",
    "- Model: EfficientNet-B0 (pretrained on ImageNet)\n",
    "- Input Size: 224x224\n",
    "- Output: 35 breed classes + confidence score\n",
    "- Final Size: ~8 MB (after INT8 quantization)\n",
    "- Target Accuracy: 85-90%\n",
    "\n",
    "**35 Indian Breeds:**\n",
    "- Cattle (23): Gir, Sahiwal, Red Sindhi, Tharparkar, Rathi, Hariana, Kankrej, Ongole, Deoni, Hallikar, Amritmahal, Khillari, Kangayam, Bargur, Dangi, Krishna Valley, Malnad Gidda, Punganur, Vechur, Pulikulam, Umblachery, Toda, Kalahandi\n",
    "- Buffalo (12): Murrah, Jaffrabadi, Nili-Ravi, Banni, Pandharpuri, Mehsana, Surti, Nagpuri, Bhadawari, Chilika, Jersey Cross, HF Cross\n",
    "\n",
    "**Author:** SIH 2025 Team\n",
    "**Problem Statement:** SIH25004 - Image-based Breed Recognition for Cattle and Buffaloes of India"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "# Install dependencies\n",
    "!pip install tensorflow==2.15.0 -q\n",
    "!pip install tensorflow-hub -q\n",
    "!pip install matplotlib seaborn -q\n",
    "!pip install scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "\n",
    "# TensorFlow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, \n",
    "    TensorBoard, CSVLogger\n",
    ")\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sklearn for metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set paths\n",
    "BASE_PATH = '/content/drive/MyDrive/cattle_breed_recognition'\n",
    "DATA_PATH = f'{BASE_PATH}/data'\n",
    "MODELS_PATH = f'{BASE_PATH}/models'\n",
    "LOGS_PATH = f'{BASE_PATH}/logs'\n",
    "\n",
    "for path in [DATA_PATH, MODELS_PATH, LOGS_PATH]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Base Path: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Breed Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 35 Indian cattle and buffalo breeds\n",
    "BREEDS = [\n",
    "    # Cattle Breeds (23)\n",
    "    'Gir', 'Sahiwal', 'Red_Sindhi', 'Tharparkar', 'Rathi',\n",
    "    'Hariana', 'Kankrej', 'Ongole', 'Deoni',\n",
    "    'Hallikar', 'Amritmahal', 'Khillari', 'Kangayam', 'Bargur',\n",
    "    'Dangi', 'Krishna_Valley', 'Malnad_Gidda', 'Punganur', 'Vechur',\n",
    "    'Pulikulam', 'Umblachery', 'Toda', 'Kalahandi',\n",
    "    # Buffalo Breeds (12)\n",
    "    'Murrah', 'Jaffrabadi', 'Nili_Ravi', 'Banni', 'Pandharpuri',\n",
    "    'Mehsana', 'Surti', 'Nagpuri', 'Bhadawari', 'Chilika',\n",
    "    # Cross Breeds (2)\n",
    "    'Jersey_Cross', 'HF_Cross'\n",
    "]\n",
    "\n",
    "NUM_CLASSES = len(BREEDS)\n",
    "print(f\"Total breeds: {NUM_CLASSES}\")\n",
    "print(f\"\\nCattle breeds: {len([b for b in BREEDS if b not in ['Murrah', 'Jaffrabadi', 'Nili_Ravi', 'Banni', 'Pandharpuri', 'Mehsana', 'Surti', 'Nagpuri', 'Bhadawari', 'Chilika', 'Jersey_Cross', 'HF_Cross']])}\")\n",
    "print(f\"Buffalo breeds: {len(['Murrah', 'Jaffrabadi', 'Nili_Ravi', 'Banni', 'Pandharpuri', 'Mehsana', 'Surti', 'Nagpuri', 'Bhadawari', 'Chilika'])}\")\n",
    "print(f\"Cross breeds: {len(['Jersey_Cross', 'HF_Cross'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset\n",
    "\n",
    "### Dataset Structure Expected:\n",
    "```\n",
    "data/\n",
    "├── train/\n",
    "│   ├── Gir/\n",
    "│   ├── Sahiwal/\n",
    "│   └── ... (35 breed folders)\n",
    "├── val/\n",
    "│   ├── Gir/\n",
    "│   └── ...\n",
    "└── test/\n",
    "    ├── Gir/\n",
    "    └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DIR = f'{DATA_PATH}/train'\n",
    "VAL_DIR = f'{DATA_PATH}/val'\n",
    "TEST_DIR = f'{DATA_PATH}/test'\n",
    "\n",
    "# Check if data exists\n",
    "if os.path.exists(TRAIN_DIR):\n",
    "    print(\"Dataset found!\")\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_path = f'{DATA_PATH}/{split}'\n",
    "        if os.path.exists(split_path):\n",
    "            total_images = sum([len(files) for r, d, files in os.walk(split_path)])\n",
    "            print(f\"  {split}: {total_images} images\")\n",
    "else:\n",
    "    print(\"Dataset not found. Please upload data to Google Drive.\")\n",
    "    print(f\"Expected path: {TRAIN_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option: Download Sample Dataset from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: Download dataset from Kaggle\n",
    "# First, upload your kaggle.json to Colab\n",
    "\n",
    "# !pip install kaggle -q\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download cattle breed dataset\n",
    "# !kaggle datasets download -d dataset-name -p {DATA_PATH}/raw\n",
    "# !unzip {DATA_PATH}/raw/dataset-name.zip -d {DATA_PATH}/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Augmentation & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=15,          # ±15 degrees\n",
    "    width_shift_range=0.1,      # Horizontal shift\n",
    "    height_shift_range=0.1,     # Vertical shift\n",
    "    shear_range=0.1,            # Shear\n",
    "    zoom_range=0.1,             # Zoom (0.9-1.1x)\n",
    "    horizontal_flip=True,       # Random horizontal flip\n",
    "    brightness_range=[0.8, 1.2], # Brightness variation\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Only rescaling for validation and test\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAIN_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    VAL_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    TEST_DIR,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Get class indices\n",
    "class_indices = train_generator.class_indices\n",
    "idx_to_class = {v: k for k, v in class_indices.items()}\n",
    "\n",
    "print(f\"\\nClass indices saved!\")\n",
    "print(f\"Number of classes: {len(class_indices)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save class indices for later use\n",
    "with open(f'{MODELS_PATH}/class_indices.json', 'w') as f:\n",
    "    json.dump(class_indices, f, indent=2)\n",
    "\n",
    "print(\"Class indices:\")\n",
    "for breed, idx in sorted(class_indices.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {idx}: {breed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented samples\n",
    "def plot_augmented_samples(generator, num_samples=5):\n",
    "    \"\"\"Plot augmented image samples.\"\"\"\n",
    "    images, labels = next(generator)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
    "    for i in range(num_samples):\n",
    "        axes[i].imshow(images[i])\n",
    "        class_idx = np.argmax(labels[i])\n",
    "        axes[i].set_title(idx_to_class[class_idx])\n",
    "        axes[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Sample augmented training images:\")\n",
    "plot_augmented_samples(train_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build EfficientNet-B0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_b0(num_classes, img_size=224, fine_tune=False):\n",
    "    \"\"\"\n",
    "    Build EfficientNet-B0 model for breed classification.\n",
    "    \n",
    "    Args:\n",
    "        num_classes: Number of breed classes\n",
    "        img_size: Input image size\n",
    "        fine_tune: Whether to unfreeze some layers for fine-tuning\n",
    "    \n",
    "    Returns:\n",
    "        Compiled Keras model\n",
    "    \"\"\"\n",
    "    # Load EfficientNet-B0 with ImageNet weights\n",
    "    base_model = EfficientNetB0(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(img_size, img_size, 3)\n",
    "    )\n",
    "    \n",
    "    # Freeze base model initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build custom head\n",
    "    inputs = keras.Input(shape=(img_size, img_size, 3))\n",
    "    \n",
    "    # Pass through base model\n",
    "    x = base_model(inputs, training=False)\n",
    "    \n",
    "    # Global average pooling\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Batch normalization\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Dense layer\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_efficientnet_b0(NUM_CLASSES, IMG_SIZE)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate model size\n",
    "def count_parameters(model):\n",
    "    trainable = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    non_trainable = sum([tf.keras.backend.count_params(w) for w in model.non_trainable_weights])\n",
    "    return trainable, non_trainable\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"\\nModel Parameters:\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable:,}\")\n",
    "print(f\"  Total: {trainable + non_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    # Save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath=f'{MODELS_PATH}/best_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Early stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # CSV logger\n",
    "    CSVLogger(f'{LOGS_PATH}/training_log.csv'),\n",
    "    \n",
    "    # TensorBoard\n",
    "    TensorBoard(log_dir=f'{LOGS_PATH}/tensorboard')\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Phase 1: Train with Frozen Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = val_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {val_generator.samples}\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Train with frozen backbone\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 1: Training with frozen EfficientNet backbone\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Phase 2: Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze top layers of EfficientNet for fine-tuning\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze first 80% of layers, fine-tune last 20%\n",
    "num_layers = len(base_model.layers)\n",
    "freeze_until = int(num_layers * 0.8)\n",
    "\n",
    "for layer in base_model.layers[:freeze_until]:\n",
    "    layer.trainable = False\n",
    "\n",
    "print(f\"Total layers in base model: {num_layers}\")\n",
    "print(f\"Frozen layers: {freeze_until}\")\n",
    "print(f\"Trainable layers: {num_layers - freeze_until}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE / 10),  # Lower LR\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")\n",
    "\n",
    "trainable, non_trainable = count_parameters(model)\n",
    "print(f\"\\nAfter unfreezing:\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "print(f\"  Non-trainable: {non_trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Fine-tuning\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 2: Fine-tuning EfficientNet layers\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Additional epochs\n",
    "    initial_epoch=history_phase1.epoch[-1] + 1,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine histories\n",
    "def combine_histories(h1, h2):\n",
    "    combined = {}\n",
    "    for key in h1.history.keys():\n",
    "        combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "history = combine_histories(history_phase1, history_phase2)\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history['accuracy'], label='Train')\n",
    "axes[0].plot(history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history['loss'], label='Train')\n",
    "axes[1].plot(history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Top-3 Accuracy\n",
    "axes[2].plot(history['top_k_categorical_accuracy'], label='Train')\n",
    "axes[2].plot(history['val_top_k_categorical_accuracy'], label='Validation')\n",
    "axes[2].set_title('Top-3 Accuracy')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{LOGS_PATH}/training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = keras.models.load_model(f'{MODELS_PATH}/best_model.keras')\n",
    "\n",
    "# Evaluate on test set\n",
    "test_generator.reset()\n",
    "test_results = best_model.evaluate(test_generator, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Test Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss: {test_results[0]:.4f}\")\n",
    "print(f\"Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"Top-3 Accuracy: {test_results[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "test_generator.reset()\n",
    "predictions = best_model.predict(test_generator, verbose=1)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "true_classes = test_generator.classes\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    true_classes, \n",
    "    predicted_classes, \n",
    "    target_names=[idx_to_class[i] for i in range(len(idx_to_class))]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "plt.figure(figsize=(20, 16))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=[idx_to_class[i] for i in range(len(idx_to_class))],\n",
    "    yticklabels=[idx_to_class[i] for i in range(len(idx_to_class))]\n",
    ")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{LOGS_PATH}/confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export to TFLite with INT8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to TFLite with INT8 quantization\n",
    "def export_to_tflite_int8(model, train_generator, output_path):\n",
    "    \"\"\"\n",
    "    Export Keras model to TFLite with INT8 quantization.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        train_generator: Training data generator for calibration\n",
    "        output_path: Path to save TFLite model\n",
    "    \"\"\"\n",
    "    # Convert to TFLite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    # Enable default optimizations (includes INT8)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    # Representative dataset for calibration\n",
    "    def representative_dataset():\n",
    "        for _ in range(100):  # 100 calibration samples\n",
    "            images, _ = next(train_generator)\n",
    "            for img in images:\n",
    "                yield [np.expand_dims(img, axis=0)]\n",
    "    \n",
    "    converter.representative_dataset = representative_dataset\n",
    "    \n",
    "    # Full INT8 quantization\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.uint8  # or tf.int8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    \n",
    "    # Convert\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"TFLite model saved to: {output_path}\")\n",
    "    print(f\"Size: {len(tflite_model) / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "# Export\n",
    "tflite_path = f'{MODELS_PATH}/efficientnet_b0_int8.tflite'\n",
    "tflite_model = export_to_tflite_int8(best_model, train_generator, tflite_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also export float16 version (alternative)\n",
    "def export_to_tflite_float16(model, output_path):\n",
    "    \"\"\"Export to TFLite with Float16 quantization.\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.target_spec.supported_types = [tf.float16]\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"TFLite Float16 model saved to: {output_path}\")\n",
    "    print(f\"Size: {len(tflite_model) / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "tflite_float16_path = f'{MODELS_PATH}/efficientnet_b0_float16.tflite'\n",
    "export_to_tflite_float16(best_model, tflite_float16_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test TFLite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TFLite model\n",
    "def test_tflite_inference(tflite_path, test_image_path, idx_to_class):\n",
    "    \"\"\"\n",
    "    Test TFLite model inference on a single image.\n",
    "    \"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    # Get input/output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(f\"Input shape: {input_details[0]['shape']}\")\n",
    "    print(f\"Input dtype: {input_details[0]['dtype']}\")\n",
    "    print(f\"Output shape: {output_details[0]['shape']}\")\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = tf.keras.preprocessing.image.load_img(\n",
    "        test_image_path, \n",
    "        target_size=(224, 224)\n",
    "    )\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    \n",
    "    # Quantize input if needed\n",
    "    if input_details[0]['dtype'] == np.uint8:\n",
    "        input_scale = input_details[0]['quantization_parameters']['scales'][0]\n",
    "        input_zero_point = input_details[0]['quantization_parameters']['zero_points'][0]\n",
    "        img_array = (img_array / input_scale + input_zero_point).astype(np.uint8)\n",
    "    \n",
    "    img_array = np.expand_dims(img_array, axis=0).astype(input_details[0]['dtype'])\n",
    "    \n",
    "    # Run inference\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    interpreter.set_tensor(input_details[0]['index'], img_array)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[0]['index'])\n",
    "    \n",
    "    inference_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    # Dequantize output if needed\n",
    "    if output_details[0]['dtype'] == np.uint8:\n",
    "        output_scale = output_details[0]['quantization_parameters']['scales'][0]\n",
    "        output_zero_point = output_details[0]['quantization_parameters']['zero_points'][0]\n",
    "        output = (output.astype(np.float32) - output_zero_point) * output_scale\n",
    "    \n",
    "    # Get top predictions\n",
    "    top_3_idx = np.argsort(output[0])[-3:][::-1]\n",
    "    \n",
    "    print(f\"\\nInference time: {inference_time:.2f} ms\")\n",
    "    print(f\"\\nTop 3 Predictions:\")\n",
    "    for idx in top_3_idx:\n",
    "        confidence = output[0][idx] * 100\n",
    "        print(f\"  {idx_to_class[idx]}: {confidence:.2f}%\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test on sample image\n",
    "test_image_path = f'{TEST_DIR}/Gir/sample_image.jpg'  # Update with actual image\n",
    "if os.path.exists(test_image_path):\n",
    "    test_tflite_inference(tflite_path, test_image_path, idx_to_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all model formats\n",
    "OUTPUT_DIR = f'{MODELS_PATH}/final'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save Keras model\n",
    "best_model.save(f'{OUTPUT_DIR}/efficientnet_b0_classifier.keras')\n",
    "\n",
    "# Save SavedModel format (for TensorFlow Serving)\n",
    "best_model.save(f'{OUTPUT_DIR}/efficientnet_b0_classifier_savedmodel')\n",
    "\n",
    "# Copy TFLite models\n",
    "shutil.copy(tflite_path, f'{OUTPUT_DIR}/efficientnet_b0_int8.tflite')\n",
    "shutil.copy(tflite_float16_path, f'{OUTPUT_DIR}/efficientnet_b0_float16.tflite')\n",
    "\n",
    "# Copy class indices\n",
    "shutil.copy(f'{MODELS_PATH}/class_indices.json', f'{OUTPUT_DIR}/class_indices.json')\n",
    "\n",
    "print(f\"Final models saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nFiles:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    fpath = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.isfile(fpath):\n",
    "        size_mb = os.path.getsize(fpath) / (1024 * 1024)\n",
    "        print(f\"  {f}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EfficientNet-B0 Classification Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nModel: EfficientNet-B0\")\n",
    "print(f\"Task: 35 Indian Breed Classification\")\n",
    "print(f\"Input Size: 224x224\")\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Test Accuracy: {test_results[1]:.4f}\")\n",
    "print(f\"  Test Top-3 Accuracy: {test_results[2]:.4f}\")\n",
    "print(f\"\\nModel Files:\")\n",
    "print(f\"  Keras: efficientnet_b0_classifier.keras\")\n",
    "print(f\"  TFLite INT8: efficientnet_b0_int8.tflite (~8 MB)\")\n",
    "print(f\"  TFLite Float16: efficientnet_b0_float16.tflite (~16 MB)\")\n",
    "print(f\"\\nReady for Integration with YOLOv8 Detection Pipeline!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
